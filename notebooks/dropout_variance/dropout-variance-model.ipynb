{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import tensorflow as tf\n",
    "print('tensorflow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concept of this notebook was from:\n",
    "# how to deal with DICOM was from\n",
    "# https://www.raddq.com/dicom-processing-segmentation-visualization-in-python/\n",
    "# and\n",
    "# https://www.kaggle.com/vgarshin/osic-keras-images-and-tabular-data-model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "## seed and defaults\n",
    "######################################################\n",
    "\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "    \n",
    "\n",
    "DATA_DIR = '../../input/osic-pulmonary-fibrosis-progression'\n",
    "GROUP_SPLITS = 5\n",
    "\n",
    "TRAINING_FEATURES = [\n",
    "    'Female', \n",
    "    'Male',\n",
    "    'Currently smokes', \n",
    "    'Ex-smoker', \n",
    "    'Never smoked',\n",
    "    'Percent',\n",
    "    #'init_week_Percent',\n",
    "    'Age', \n",
    "    'relative_week', \n",
    "    'init_week_FVC'\n",
    "]\n",
    "\n",
    "SCALED_FEATURES = [\n",
    "    'Percent', \n",
    "    'Age',\n",
    "    'relative_week', \n",
    "    'init_week_FVC'\n",
    "]\n",
    "\n",
    "IMG_SIZE = 224\n",
    "IMG_SLICES = 12\n",
    "CUTOFF = 2\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 10\n",
    "BATCH_PRED = 1\n",
    "MODEL_NAME = 'dropout_variance'\n",
    "MODEL_VERSION = 'v11b'\n",
    "MODEL = MODEL_NAME + '_' + MODEL_VERSION + '_batch_' + str(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "## get files and split tabular data\n",
    "######################################################\n",
    "\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "train.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\n",
    "\n",
    "test = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "subm = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\n",
    "\n",
    "subm['Patient'] = subm['Patient_Week'].apply(lambda x: x.split('_')[0])\n",
    "subm['Weeks'] = subm['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "subm =  subm[['Patient','Weeks','Confidence','Patient_Week']]\n",
    "subm = subm.merge(test.drop('Weeks', axis=1), on='Patient')\n",
    "\n",
    "train['SPLIT'] = 'train'\n",
    "test['SPLIT'] = 'test'\n",
    "subm['SPLIT'] = 'submission'\n",
    "\n",
    "data = train.append([test, subm])\n",
    "\n",
    "######################################################\n",
    "## initial week and relative week augmentations\n",
    "######################################################\n",
    "\n",
    "data['init_week'] = data['Weeks']\n",
    "data.loc[data.SPLIT == 'submission', 'init_week'] = np.nan\n",
    "data['init_week'] = data.groupby('Patient')['init_week'].transform('min')\n",
    "data['relative_week'] = data['Weeks'] - data['init_week']\n",
    "\n",
    "######################################################\n",
    "## add initial fvc to all patients rows\n",
    "######################################################\n",
    "\n",
    "init_fvc = data.groupby('Patient')[['Patient', 'Weeks', 'init_week', 'FVC']].head()\n",
    "init_fvc = init_fvc.loc[init_fvc.Weeks == init_fvc.init_week]\n",
    "init_fvc.columns = ['Patient', 'Weeks', 'init_week', 'init_week_FVC']\n",
    "init_fvc.drop(['Weeks', 'init_week'], axis=1, inplace=True)\n",
    "data = data.merge(init_fvc, on='Patient', how='left')\n",
    "\n",
    "del init_fvc\n",
    "\n",
    "\n",
    "######################################################\n",
    "## scale the continuous variables\n",
    "## and dummies of categories\n",
    "######################################################\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "data[SCALED_FEATURES] = min_max_scaler.fit_transform(data[SCALED_FEATURES])\n",
    "data = pd.concat([data, pd.get_dummies(data.Sex), pd.get_dummies(data.SmokingStatus)], axis=1)\n",
    "\n",
    "######################################################\n",
    "## add initial percent to all patients rows\n",
    "######################################################\n",
    "\n",
    "init_perc = data.groupby('Patient')[['Patient', 'Weeks', 'init_week', 'Percent']].head()\n",
    "init_perc = init_perc.loc[init_perc.Weeks == init_perc.init_week]\n",
    "init_perc.columns = ['Patient', 'Weeks', 'init_week', 'init_week_Percent']\n",
    "init_perc.drop(['Weeks', 'init_week'], axis=1, inplace=True)\n",
    "data = data.merge(init_perc, on='Patient', how='left')\n",
    "\n",
    "del init_perc\n",
    "\n",
    "######################################################\n",
    "## separate for training, testing, submission\n",
    "######################################################\n",
    "\n",
    "train = data.loc[data.SPLIT == 'train']\n",
    "test = data.loc[data.SPLIT == 'test']\n",
    "subm = data.loc[data.SPLIT == 'submission']\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Weeks</th>\n",
       "      <th>FVC</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SmokingStatus</th>\n",
       "      <th>SPLIT</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Patient_Week</th>\n",
       "      <th>init_week</th>\n",
       "      <th>relative_week</th>\n",
       "      <th>init_week_FVC</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>Currently smokes</th>\n",
       "      <th>Ex-smoker</th>\n",
       "      <th>Never smoked</th>\n",
       "      <th>init_week_Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>-4</td>\n",
       "      <td>2315</td>\n",
       "      <td>0.236393</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.179012</td>\n",
       "      <td>0.241456</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>5</td>\n",
       "      <td>2214</td>\n",
       "      <td>0.215941</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.234568</td>\n",
       "      <td>0.241456</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>7</td>\n",
       "      <td>2061</td>\n",
       "      <td>0.184960</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.246914</td>\n",
       "      <td>0.241456</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>9</td>\n",
       "      <td>2144</td>\n",
       "      <td>0.201767</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.241456</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>11</td>\n",
       "      <td>2069</td>\n",
       "      <td>0.186580</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.271605</td>\n",
       "      <td>0.241456</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Patient  Weeks   FVC   Percent       Age   Sex  \\\n",
       "0  ID00007637202177411956430     -4  2315  0.236393  0.769231  Male   \n",
       "1  ID00007637202177411956430      5  2214  0.215941  0.769231  Male   \n",
       "2  ID00007637202177411956430      7  2061  0.184960  0.769231  Male   \n",
       "3  ID00007637202177411956430      9  2144  0.201767  0.769231  Male   \n",
       "4  ID00007637202177411956430     11  2069  0.186580  0.769231  Male   \n",
       "\n",
       "  SmokingStatus  SPLIT  Confidence Patient_Week  init_week  relative_week  \\\n",
       "0     Ex-smoker  train         NaN          NaN       -4.0       0.179012   \n",
       "1     Ex-smoker  train         NaN          NaN       -4.0       0.234568   \n",
       "2     Ex-smoker  train         NaN          NaN       -4.0       0.246914   \n",
       "3     Ex-smoker  train         NaN          NaN       -4.0       0.259259   \n",
       "4     Ex-smoker  train         NaN          NaN       -4.0       0.271605   \n",
       "\n",
       "   init_week_FVC  Female  Male  Currently smokes  Ex-smoker  Never smoked  \\\n",
       "0       0.241456       0     1                 0          1             0   \n",
       "1       0.241456       0     1                 0          1             0   \n",
       "2       0.241456       0     1                 0          1             0   \n",
       "3       0.241456       0     1                 0          1             0   \n",
       "4       0.241456       0     1                 0          1             0   \n",
       "\n",
       "   init_week_Percent  \n",
       "0           0.236393  \n",
       "1           0.236393  \n",
       "2           0.236393  \n",
       "3           0.236393  \n",
       "4           0.236393  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### image helpers\n",
    "def get_img_seq(pat_id, slice_count, data_dir, folder, img_size):\n",
    "        \n",
    "    images = []\n",
    "\n",
    "    slices = get_images(pat_id, slice_count, data_dir, folder)\n",
    "    scans = get_pixels_hu(slices)\n",
    "\n",
    "    for img_idx in range(slice_count):\n",
    "        img = scans[img_idx]\n",
    "\n",
    "        ## resize images to be same shape\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "\n",
    "        ## normalize the image pixels\n",
    "        img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "\n",
    "        #reshape for tesnor\n",
    "        img = np.repeat(img[..., np.newaxis], 3, -1)\n",
    "        images.append(img)     \n",
    "\n",
    "    return np.array(images).astype(np.float32)\n",
    "    \n",
    "def get_pixels_hu(scans):\n",
    "    '''\n",
    "    hu pixel is from\n",
    "    https://www.raddq.com/dicom-processing-segmentation-visualization-in-python/\n",
    "    '''\n",
    "    \n",
    "    image = np.stack([s.pixel_array for s in scans])\n",
    "    # Convert to int16 (from sometimes int16), \n",
    "    # should be possible as values should always be low enough (<32k)\n",
    "    image = image.astype(np.int16)\n",
    "\n",
    "    # Set outside-of-scan pixels to 1\n",
    "    # The intercept is usually -1024, so air is approximately 0\n",
    "    image[image == -2000] = 0\n",
    "    \n",
    "    # Convert to Hounsfield units (HU)\n",
    "    intercept = scans[0].RescaleIntercept\n",
    "    slope = scans[0].RescaleSlope\n",
    "    \n",
    "    if slope != 1:\n",
    "        image = slope * image.astype(np.float64)\n",
    "        image = image.astype(np.int16)\n",
    "        \n",
    "    image += np.int16(intercept)\n",
    "    \n",
    "    return np.array(image, dtype=np.int16)\n",
    "\n",
    "\n",
    "def get_images(pat_id, slice_count, data_dir, folder):\n",
    "    c_off = 2\n",
    "    path = f'{data_dir}/{folder}/{pat_id}'\n",
    "\n",
    "    file_names = sorted(os.listdir(path), key=lambda x: int(os.path.splitext(x)[0]))\n",
    "\n",
    "    idxs = [\n",
    "        int(i * len(file_names) / (slice_count + 2 * c_off)) \n",
    "        for i in range(slice_count + 2 * c_off)\n",
    "    ]\n",
    "\n",
    "    image_array = [\n",
    "        pydicom.read_file(path + '/' + file_names[idx])\n",
    "        for idx in idxs[c_off:-c_off]\n",
    "    ]\n",
    "\n",
    "    if len(image_array) < slice_count:\n",
    "        for i in range(slice_count - len(image_array)):\n",
    "            image_array.append(pydicom.read_file(path + '/' + os.listdir(path)[-1]))\n",
    "\n",
    "    return image_array\n",
    "\n",
    "######################################################\n",
    "## data generator, used to feed data to TensorFlow in batches\n",
    "######################################################\n",
    "\n",
    "class DataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(\n",
    "        self, \n",
    "        df, \n",
    "        tab_features,\n",
    "        data_dir,\n",
    "        batch_size=8, \n",
    "        mode='fit', \n",
    "        shuffle=False, \n",
    "        cutoff=2,\n",
    "        folder='train',\n",
    "        slice_count=12, \n",
    "        img_size=224):\n",
    "\n",
    "        self.df = df\n",
    "        self.data_dir = data_dir\n",
    "        self.shuffle = shuffle\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.folder = folder\n",
    "        self.img_size = img_size\n",
    "        self.slice_count = slice_count\n",
    "        self.tab_features = tab_features\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        return int(np.floor(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        self.indexes = np.arange(len(self.df))\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        batch_size = min(self.batch_size, len(self.df) - index * self.batch_size)\n",
    "        \n",
    "        X_img = np.zeros((batch_size, self.slice_count, self.img_size, self.img_size, 3), dtype=np.float32)\n",
    "        X_tab = self.df[index * self.batch_size : (index + 1) * self.batch_size][self.tab_features].values\n",
    "        pats_batch = self.df[index * self.batch_size : (index + 1) * self.batch_size]['Patient'].values\n",
    "        \n",
    "        for i, pat_id in enumerate(pats_batch):\n",
    "            imgs_seq = get_img_seq(pat_id, self.slice_count, self.data_dir, self.folder, self.img_size)\n",
    "            X_img[i, ] = imgs_seq\n",
    "\n",
    "        if self.mode == 'fit' or self.mode == 'test':\n",
    "            y = np.array(\n",
    "                self.df[index * self.batch_size : (index + 1) * self.batch_size]['FVC'].values, \n",
    "                dtype=np.float32\n",
    "            )\n",
    "\n",
    "            return (X_img, X_tab), y\n",
    "\n",
    "        elif self.mode == 'predict':\n",
    "            y = np.zeros(batch_size, dtype=np.float32)\n",
    "\n",
    "            return (X_img, X_tab), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "## Custom model\n",
    "## cnn for images and dnn for tabs\n",
    "######################################################\n",
    "\n",
    "def get_cnn(inputs_image_shape, dropout_prob, nodes, node_increase, seed):\n",
    "    print('dropout prob:', dropout_prob, 'nodes:', nodes, 'node increase:', node_increase)\n",
    "    \n",
    "    inputs_images = tf.keras.layers.Input(shape=(*inputs_image_shape, ))\n",
    "    x_images = tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization())(inputs_images)\n",
    "    x_images = tf.keras.layers.GlobalMaxPooling3D()(x_images)\n",
    "    x_images = tf.keras.layers.BatchNormalization()(x_images)\n",
    "    x_images = tf.keras.layers.Dropout(dropout_prob, seed=seed)(x_images)  \n",
    "    x_images = tf.keras.layers.Dense(int(node_increase * nodes), activation='relu')(x_images)\n",
    "    x_images = tf.keras.layers.BatchNormalization()(x_images)\n",
    "    x_images = tf.keras.layers.Dropout(dropout_prob, seed=seed)(x_images)\n",
    "    x_images = tf.keras.layers.Dense(nodes, activation='relu')(x_images)\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs_images, x_images)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_dnn(inputs_tab_shape, nodes):\n",
    "    \n",
    "    inputs_tab = tf.keras.layers.Input(shape=(inputs_tab_shape, ))\n",
    "    x_tabs = tf.keras.layers.Dense(nodes, activation='relu')(inputs_tab)\n",
    "\n",
    "    model = tf.keras.Model(inputs_tab, x_tabs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_model(inputs_image_shape, inputs_tab_shape, seed, dropout_prob=.05, nodes=4, node_increase=2, lr=0.1):\n",
    "\n",
    "    cnn = get_cnn(inputs_image_shape, dropout_prob=dropout_prob, nodes=nodes, node_increase=node_increase, seed=seed)\n",
    "    dnn = get_dnn(inputs_tab_shape, nodes=nodes)\n",
    "    \n",
    "    combinedInput = tf.keras.layers.concatenate([cnn.output, dnn.output])\n",
    "    \n",
    "    x = tf.keras.layers.Dense(int(nodes/2), activation=\"relu\")(combinedInput)\n",
    "    preds = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[cnn.input, dnn.input], outputs=preds)\n",
    "    \n",
    "    opt = tf.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "## Since we have groups (patients) this splits\n",
    "## in a way we don't mix a patient in validation\n",
    "## and in training at same time\n",
    "######################################################\n",
    "\n",
    "group_folds = GroupKFold(n_splits=GROUP_SPLITS)\n",
    "train['fold'] = -1\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(group_folds.split(train, groups=train['Patient'])):\n",
    "    train.loc[val_idx, 'fold'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout prob: 0.7 nodes: 4 node increase: 2\n",
      "Epoch 1/30\n",
      "  1/122 [..............................] - ETA: 0s - loss: 2325.1968WARNING:tensorflow:From /home/dev_pool/anaconda3/envs/thesis/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "122/122 [==============================] - ETA: 0s - loss: 1328.6523\n",
      "Epoch 00001: val_loss improved from inf to 507.76364, saving model to dropout_variance_v11b_batch_10_fold_0.h5\n",
      "122/122 [==============================] - 64s 525ms/step - loss: 1328.6523 - val_loss: 507.7636\n",
      "Epoch 2/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 478.3312\n",
      "Epoch 00002: val_loss improved from 507.76364 to 391.12793, saving model to dropout_variance_v11b_batch_10_fold_0.h5\n",
      "122/122 [==============================] - 63s 518ms/step - loss: 478.3312 - val_loss: 391.1279\n",
      "Epoch 3/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 329.9251\n",
      "Epoch 00003: val_loss improved from 391.12793 to 257.04163, saving model to dropout_variance_v11b_batch_10_fold_0.h5\n",
      "122/122 [==============================] - 63s 519ms/step - loss: 329.9251 - val_loss: 257.0416\n",
      "Epoch 4/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 230.2243\n",
      "Epoch 00004: val_loss improved from 257.04163 to 162.72087, saving model to dropout_variance_v11b_batch_10_fold_0.h5\n",
      "122/122 [==============================] - 63s 518ms/step - loss: 230.2243 - val_loss: 162.7209\n",
      "Epoch 5/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 188.6120\n",
      "Epoch 00005: val_loss did not improve from 162.72087\n",
      "122/122 [==============================] - 63s 517ms/step - loss: 188.6120 - val_loss: 272.7600\n",
      "Epoch 6/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 201.5716\n",
      "Epoch 00006: val_loss improved from 162.72087 to 126.10407, saving model to dropout_variance_v11b_batch_10_fold_0.h5\n",
      "122/122 [==============================] - 63s 519ms/step - loss: 201.5716 - val_loss: 126.1041\n",
      "Epoch 7/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 184.6791\n",
      "Epoch 00007: val_loss improved from 126.10407 to 116.22233, saving model to dropout_variance_v11b_batch_10_fold_0.h5\n",
      "122/122 [==============================] - 64s 523ms/step - loss: 184.6791 - val_loss: 116.2223\n",
      "Epoch 8/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 178.1818\n",
      "Epoch 00008: val_loss did not improve from 116.22233\n",
      "122/122 [==============================] - 64s 521ms/step - loss: 178.1818 - val_loss: 148.8098\n",
      "Epoch 9/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 178.4267\n",
      "Epoch 00009: val_loss did not improve from 116.22233\n",
      "122/122 [==============================] - 63s 520ms/step - loss: 178.4267 - val_loss: 128.0343\n",
      "Epoch 10/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 165.4421\n",
      "Epoch 00010: val_loss did not improve from 116.22233\n",
      "122/122 [==============================] - 63s 518ms/step - loss: 165.4421 - val_loss: 121.7999\n",
      "Epoch 11/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 158.1848\n",
      "Epoch 00011: val_loss did not improve from 116.22233\n",
      "122/122 [==============================] - 63s 519ms/step - loss: 158.1848 - val_loss: 146.2240\n",
      "Epoch 12/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 162.5615\n",
      "Epoch 00012: val_loss did not improve from 116.22233\n",
      "122/122 [==============================] - 64s 521ms/step - loss: 162.5615 - val_loss: 148.6036\n",
      "dropout prob: 0.5 nodes: 4 node increase: 2\n",
      "Epoch 1/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 1060.9816\n",
      "Epoch 00001: val_loss improved from inf to 794.24768, saving model to dropout_variance_v11b_batch_10_fold_1.h5\n",
      "122/122 [==============================] - 64s 525ms/step - loss: 1060.9816 - val_loss: 794.2477\n",
      "Epoch 2/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 377.1382\n",
      "Epoch 00002: val_loss improved from 794.24768 to 431.82950, saving model to dropout_variance_v11b_batch_10_fold_1.h5\n",
      "122/122 [==============================] - 63s 520ms/step - loss: 377.1382 - val_loss: 431.8295\n",
      "Epoch 3/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 268.2750\n",
      "Epoch 00003: val_loss improved from 431.82950 to 287.20816, saving model to dropout_variance_v11b_batch_10_fold_1.h5\n",
      "122/122 [==============================] - 64s 521ms/step - loss: 268.2750 - val_loss: 287.2082\n",
      "Epoch 4/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 200.3796\n",
      "Epoch 00004: val_loss improved from 287.20816 to 224.28844, saving model to dropout_variance_v11b_batch_10_fold_1.h5\n",
      "122/122 [==============================] - 64s 521ms/step - loss: 200.3796 - val_loss: 224.2884\n",
      "Epoch 5/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 167.4810\n",
      "Epoch 00005: val_loss did not improve from 224.28844\n",
      "122/122 [==============================] - 64s 522ms/step - loss: 167.4810 - val_loss: 454.0374\n",
      "Epoch 6/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 155.7264\n",
      "Epoch 00006: val_loss did not improve from 224.28844\n",
      "122/122 [==============================] - 64s 523ms/step - loss: 155.7264 - val_loss: 673.0596\n",
      "Epoch 7/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 170.7177\n",
      "Epoch 00007: val_loss did not improve from 224.28844\n",
      "122/122 [==============================] - 64s 523ms/step - loss: 170.7177 - val_loss: 489.3140\n",
      "Epoch 8/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 168.7346\n",
      "Epoch 00008: val_loss did not improve from 224.28844\n",
      "122/122 [==============================] - 63s 520ms/step - loss: 168.7346 - val_loss: 411.0121\n",
      "Epoch 9/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 167.0784\n",
      "Epoch 00009: val_loss did not improve from 224.28844\n",
      "122/122 [==============================] - 63s 520ms/step - loss: 167.0784 - val_loss: 454.2835\n",
      "dropout prob: 0.1 nodes: 4 node increase: 2\n",
      "Epoch 1/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 1022.1757\n",
      "Epoch 00001: val_loss improved from inf to 1227.10400, saving model to dropout_variance_v11b_batch_10_fold_2.h5\n",
      "122/122 [==============================] - 66s 539ms/step - loss: 1022.1757 - val_loss: 1227.1040\n",
      "Epoch 2/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 407.4682\n",
      "Epoch 00002: val_loss improved from 1227.10400 to 390.60803, saving model to dropout_variance_v11b_batch_10_fold_2.h5\n",
      "122/122 [==============================] - 63s 517ms/step - loss: 407.4682 - val_loss: 390.6080\n",
      "Epoch 3/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 286.1324\n",
      "Epoch 00003: val_loss improved from 390.60803 to 277.26425, saving model to dropout_variance_v11b_batch_10_fold_2.h5\n",
      "122/122 [==============================] - 63s 518ms/step - loss: 286.1324 - val_loss: 277.2643\n",
      "Epoch 4/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 192.0301\n",
      "Epoch 00004: val_loss improved from 277.26425 to 217.81297, saving model to dropout_variance_v11b_batch_10_fold_2.h5\n",
      "122/122 [==============================] - 63s 520ms/step - loss: 192.0301 - val_loss: 217.8130\n",
      "Epoch 5/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 176.3431\n",
      "Epoch 00005: val_loss did not improve from 217.81297\n",
      "122/122 [==============================] - 63s 518ms/step - loss: 176.3431 - val_loss: 227.4253\n",
      "Epoch 6/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 173.2065\n",
      "Epoch 00006: val_loss did not improve from 217.81297\n",
      "122/122 [==============================] - 63s 518ms/step - loss: 173.2065 - val_loss: 349.2691\n",
      "Epoch 7/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 184.4808\n",
      "Epoch 00007: val_loss improved from 217.81297 to 212.15912, saving model to dropout_variance_v11b_batch_10_fold_2.h5\n",
      "122/122 [==============================] - 63s 520ms/step - loss: 184.4808 - val_loss: 212.1591\n",
      "Epoch 8/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 179.0409\n",
      "Epoch 00008: val_loss did not improve from 212.15912\n",
      "122/122 [==============================] - 63s 517ms/step - loss: 179.0409 - val_loss: 351.4854\n",
      "Epoch 9/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 170.9517\n",
      "Epoch 00009: val_loss improved from 212.15912 to 153.99780, saving model to dropout_variance_v11b_batch_10_fold_2.h5\n",
      "122/122 [==============================] - 63s 518ms/step - loss: 170.9517 - val_loss: 153.9978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 175.6600\n",
      "Epoch 00010: val_loss did not improve from 153.99780\n",
      "122/122 [==============================] - 63s 518ms/step - loss: 175.6600 - val_loss: 155.8015\n",
      "Epoch 11/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 160.0197\n",
      "Epoch 00011: val_loss did not improve from 153.99780\n",
      "122/122 [==============================] - 63s 517ms/step - loss: 160.0197 - val_loss: 158.1008\n",
      "Epoch 12/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 160.9510\n",
      "Epoch 00012: val_loss did not improve from 153.99780\n",
      "122/122 [==============================] - 63s 514ms/step - loss: 160.9510 - val_loss: 294.1799\n",
      "Epoch 13/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 151.3443\n",
      "Epoch 00013: val_loss did not improve from 153.99780\n",
      "122/122 [==============================] - 63s 517ms/step - loss: 151.3443 - val_loss: 168.7215\n",
      "Epoch 14/30\n",
      "122/122 [==============================] - ETA: 0s - loss: 169.3071\n",
      "Epoch 00014: val_loss did not improve from 153.99780\n",
      "122/122 [==============================] - 63s 518ms/step - loss: 169.3071 - val_loss: 205.0640\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "## finally the training\n",
    "######################################################\n",
    "## model tuning\n",
    "## hyperparam that can be tuned\n",
    "dropout = [0.7, 0.5, 0.1]\n",
    "nodes = 4\n",
    "node_increase = 2\n",
    "lr = 0.1\n",
    "\n",
    "folds_history = []\n",
    "\n",
    "for i in range(len(dropout)):\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5, \n",
    "        verbose=0,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    checkpoing_save = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f'{MODEL}_fold_{i}.h5', \n",
    "        monitor='val_loss', \n",
    "        verbose=1, \n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    log_dir = \"logs/fit/\" + MODEL + '_fold_' + str(i) + '/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "    train_datagen = DataGen(\n",
    "        df=train.loc[train['fold'] != i], \n",
    "        tab_features=TRAINING_FEATURES,\n",
    "        data_dir=DATA_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        mode='fit', \n",
    "        shuffle=True, \n",
    "        folder='train',\n",
    "        slice_count=IMG_SLICES, \n",
    "        img_size=IMG_SIZE\n",
    "    )\n",
    "\n",
    "    val_datagen = DataGen(\n",
    "        df=train.loc[train['fold'] == i],\n",
    "        tab_features=TRAINING_FEATURES,\n",
    "        data_dir=DATA_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        mode='fit', \n",
    "        shuffle=False, \n",
    "        folder='train',\n",
    "        slice_count=IMG_SLICES, \n",
    "        img_size=IMG_SIZE\n",
    "    )\n",
    "    \n",
    "    model = get_model(\n",
    "        inputs_image_shape=(IMG_SLICES, IMG_SIZE, IMG_SIZE, 3), \n",
    "        inputs_tab_shape=len(TRAINING_FEATURES),\n",
    "        seed=seed,\n",
    "        dropout_prob=dropout[i], \n",
    "        nodes=nodes,\n",
    "        node_increase=node_increase, \n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_datagen,\n",
    "        validation_data=val_datagen,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        epochs=EPOCHS, \n",
    "        callbacks=[checkpoing_save, early_stop, tensorboard_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    folds_history.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dropout)):\n",
    "    h = folds_history[i]\n",
    "    history_file = f'{MODEL}_fold_{i}_history.txt'\n",
    "    dict_to_save = {}\n",
    "\n",
    "    for k, v in h.history.items():\n",
    "        dict_to_save.update({k: [np.format_float_positional(x) for x in h.history[k]]})\n",
    "\n",
    "    with open(history_file, 'w') as file:\n",
    "        json.dump(dict_to_save, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = f'{MODEL}_fold_{0}.h5'\n",
    "model = tf.keras.models.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 12, 224, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 12, 224, 224, 12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling3d (GlobalMax (None, 3)            0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 3)            12          global_max_pooling3d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 3)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 8)            32          dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 8)            32          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8)            0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 9)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            36          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            40          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8)            0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            18          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            3           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 185\n",
      "Trainable params: 157\n",
      "Non-trainable params: 28\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
